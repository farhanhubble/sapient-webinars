{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unsupervised Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/farhanhubble/sapient-webinars/blob/master/Unsupervised_Learning.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "g-Xht7-ba6AU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Unsupervised Learning using Matrix Decomposition\n",
        "\n",
        "Matrix decomposition is a family of powerful unsupervised learning methods. Some examples are principal component analysis (PCA), singlular value decomposition (SVD), independent component analysis (ICA), independent vector ananlysis (IVA), non-negative matrix factorization (NMF) "
      ]
    },
    {
      "metadata": {
        "id": "kObdTx3OrEmS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Principal Component Ananlysis \n",
        "\n",
        "**Motivation:**\n",
        "\n",
        "- If every instance in a dataset has $n$ features or equivalently, it is a $n$-dimensional vector, can each instance be approximately represented as a $m$-dimensional vector ($m<<n$)? \n",
        "\n",
        "\n",
        "**Question:**\n",
        "\n",
        "- If such a representation is possible what is a good choice for the $m$ new dimensions?\n",
        "\n",
        "**Clues:** \n",
        "In most cases dimensions are not totally independent so more than one dimensions can be rolled into one. \n",
        "\n",
        "\n",
        "**Examples:** \n",
        "1. Consider a dataset containing 10s audio clips (e.g. Youtube8M). The features here are made up of  the amplitude of the audio recorded (say) 44100/s. So every clip will be a vector of 44100 x 10 = 441000 or the dataset  could be said to be 441000-dimensional. \n",
        "\n",
        "Because of the continuous nature of audio, nearby samples will be similar in magnitude.\n",
        "\n",
        "\n",
        "2. Consider 1000px x 1000px images. When flattened, every image hass 1,000,000 features but because of the 2D nature of images every pixel will have an intensity that is very likely to be similar to its neighboring pixels. When the image is flattened, nearby pixels get distributed to far away indices but the similarity remains.\n",
        "\n",
        "\n",
        "In both cases, there is substantial amount of redundancy and the dimension is very large. If we just want to reduce the dimension (at the expense of some information loss), we could subsample the data. For example audio can be resampled to 16000 samples/s or image can be resampled to 400px * 400px. The subsampled data will still have the redundancy mentioned earlier.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "cpdEQi5ovYGd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Redundancy can be caused by multiple sources:\n",
        "\n",
        "- If we know the extent of redundancy and the features that are redundant, we can perhaps combine them using some simple strategy, for example, replacing many redundant features with their mean. However taking means is a local operation, applied to every instance in the dataset, it does not benefit from the information available about the redundancy across all instances.\n",
        "\n",
        "- Very often we do not know which features are redundant, for example [this tutorial](https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf) gives the example of an object that actually is constrained to move in a 1d line, but whose position is being captured by 3 cameras, each of which records the position of the object on some 2d space. This produces 6 measurements or features for otherwise what would have been described with just one feature. \n",
        "\n",
        "![1D spring mass system](https://snag.gy/hmrcH5.jpg)\n",
        "\n",
        "\n",
        "**By looking at all values of a feature in a dataset, we can better estimate the redundancy in the dataset. Every feature can be treated as a distribution.**"
      ]
    },
    {
      "metadata": {
        "id": "gCrrNVpW3Jwg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Measuring Redundancy\n",
        "- Measuring redundancy requires a definition of redundancy \n",
        "\n",
        " 1. If the values in a column are nearly a multiple of values in another column, are the two redundant?\n",
        " 2. If the values in a column are nearly the squares of values in another column, are they redundant?\n",
        " 3. If the values in a column are ratios of values in two other columns?\n",
        " \n",
        " It turns out that redundancy can only be defined based on the problem at hand and our model of the solution.\n",
        " \n",
        "** Example:** Suppose you are trying to build a system that predicts the trajectory of a football. You are estimating the position, velocity, spin rate  and direction of motion of the ball  from a sequence of video frames and then using these as features to predict the trajectory of the ball. Should you model use just the velocity or velocity squared?\n",
        "\n",
        "It turns out that motion in the presence of air drag depends on the square of the velocity too and hence a model that takes this (domain knowledge) into account and uses what appears to be redundant information will likely outperform a model that uses only the velocity as a feature.\n",
        "\n",
        "\n",
        "### Lessons\n",
        "- ** Not only can redundancy be desirable sometimes, it is generally impossible to find redundancy of a general nature. **\n",
        "\n",
        "\n",
        "Keeping this in mind we set our sight on removing the simplest type of redundancy that is both undesirable as well as easy to detect and that is : ** Covariance / Correlation **"
      ]
    },
    {
      "metadata": {
        "id": "0OCry2wzBipi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Covariance\n",
        "The covariance of two  (feature) vectors $ \\vec v_1, \\vec v_2 $ is their dot product normalized by their length :  $Cov(v_1,v_2)  = {{{(\\vec v_1 - \\bar v_1)} \\cdot {(\\vec v_2 - \\bar v_2)}} \\over n}$ where $\\bar \\cdot$ indicates the mean. This is similar to the variance of a single vector, that can be defined as  $Var(v_1) = Cov(v_1,v_1) = {{{(\\vec v_1 - \\bar v_1)} \\cdot {(\\vec v_1 - \\bar v_1)}} \\over n}$. \n",
        "\n",
        "\n",
        "### Correlation\n",
        "The correlation of two vectors is their covariance scaled by the product of their standard deviations defined as $Cor(v_1, v_2) = {Cov(v_1, v_2) \\over {\\sqrt {Var(v_1)} \\sqrt {Var(v_2)} }}$ = $Cor(v_1, v_2) = {Cov(v_1, v_2) \\over {\\sqrt {Cov(v_1,v_1)} \\sqrt {Cov(v_2,v_2)} }}$ \n",
        "\n",
        "\n",
        "** It is easy to see that the covariance or correlation is zero if the two vectors are orthogonal and non-zero otherwise. **\n",
        "\n",
        "The covariance of $n$ feature vectors is a $n \\times n$ matrix of the form:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "Cov(v_1,v_1) && Cov(v_1,v_2) && ... && Cov(v_1,v_n) \\\\\n",
        "Cov(v_2,v_1) && Cov(v_2,v_2) && ... && Cov(v_2,v_n) \\\\\n",
        ". && . && ... &&. \\\\\n",
        ". && . && ... &&. \\\\\n",
        ". && . && ... &&. \\\\\n",
        "Cov(v_n,v_1) && Cov(v_n,v_2) && ... && Cov(v_n,v_n) \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "** The idea of Principal Component Analysis is to take a covariance (or correlation) matrix of a dataset and decompose it into  a product of three matrices $U*D*U^T$ ,where the matrix $D$ is nearly diagonal. The columns of matrix U are new dimensions (unit vectors) onto which the old features  can be projected. These unit vectors are called the principal components or eigen-vectors and the diagonal values in D are called the eigenvalues. Since the off-diagonal elements are nearly zero, the eigen vectors have almost zero covariance. **  \n"
      ]
    },
    {
      "metadata": {
        "id": "EEF76kgmXv5K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UlNgF5DDboeN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "v1 = [1,2,3]\n",
        "v2 = [4,7,9]\n",
        "m1 = np.mean(v1)\n",
        "m2 = np.mean(v2)\n",
        "print(v1-m1, v2-m2)\n",
        "np.cov(v1,v2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n8eupq9nbwOd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "v1 = [1,2,3]\n",
        "v2 = [5,1,-2]\n",
        "m1 = np.mean(v1)\n",
        "m2 = np.mean(v2)\n",
        "print(v1-m1, v2-m2)\n",
        "np.cov(v1,v2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LZLOW9mtb764",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "v1 = [1,2,3]\n",
        "v2 = [0.66,0,0.66]\n",
        "m1 = np.mean(v1)\n",
        "m2 = np.mean(v2)\n",
        "print(v1-m1, v2-m2)\n",
        "np.cov(v1,v2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lSOrQ2Cxlo-N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Geometric Interpretation of PCA\n",
        "Geometrically, PCA gives us new dimensions that maximize variance while minimizing covariance. \n",
        "\n",
        "![variance](https://snag.gy/5jQWKe.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "t98vdfmM-6Mn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Applications of PCA\n",
        "\n",
        "- Decorrelation\n",
        "- Feature selection \n",
        "- Dimensionality Reduction \n",
        "\n",
        "\n",
        "### Caveats\n",
        "- PCA is lossy.\n",
        "- PCA does not take into account the labels (that's why it is unsupervised) and may make classification harder."
      ]
    },
    {
      "metadata": {
        "id": "TgZIW_WGTVgN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### The Fashion MNIST dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z2wFApFp7TyA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from urllib import request\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0-ohCax07VJa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "request.urlretrieve('https://github.com/farhanhubble/sapient-webinars/raw/master/fashion_train.npy','./fashion_train.npy')\n",
        "request.urlretrieve('https://github.com/farhanhubble/sapient-webinars/raw/master/fashion_test.npy','./fashion_test.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wL1AMy32A7pi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fashion_train = np.load('fashion_train.npy')\n",
        "fashion_test = np.load('fashion_test.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vBD9w0G9BH5A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fashion_train_X , fashion_train_y = fashion_train[:,1:] , fashion_train[:,0]\n",
        "fashion_test_X , fashion_test_y = fashion_test[:,1:] , fashion_test[:,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vU0-DJudThif",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "  plt.subplot(4,5,i+1)\n",
        "  plt.imshow(fashion_train_X[i].reshape([28,28]), cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HBiTKX-UDZId",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TE7iFv4yE7AN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pca = PCA(0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "peg5n8s0aoHl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fashion_train_X_pca = pca.fit_transform(fashion_train_X)\n",
        "fashion_test_X_pca = pca.transform(fashion_test_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JhEH21ukyFjK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Note that the principal components returned by Sklearn are stored row-wise\n",
        "## So there are 81 vecctors each of length 784.\n",
        "print(pca.components_.shape)\n",
        "print(pca.n_components_, np.cumsum(pca.explained_variance_ratio_))\n",
        "                                   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5DvqL3uRqdAE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Eyeballing redundancy in Fashion MNIST\n"
      ]
    },
    {
      "metadata": {
        "id": "xLavuu17q20c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Measure covariance across columns\n",
        "\n",
        "## Covariance of original data.\n",
        "plt.imshow(np.cov(fashion_train_X.T),cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "## Covariance of transformed data columns (first 20 only)\n",
        "plt.imshow(np.cov(fashion_train_X_pca.T[:10]),cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "## Covariance of transformed data columns \n",
        "plt.imshow(np.cov(fashion_train_X_pca.T),cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ILgzR5FSRoyO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Plot the first few principal components"
      ]
    },
    {
      "metadata": {
        "id": "by5jFnoCOVEW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "  plt.subplot(4,5,i+1)\n",
        "  plt.imshow(pca.components_[i].reshape([28,28]), cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZSRp0ACGERbm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Applying PCA based dimensionality reduction to classsification"
      ]
    },
    {
      "metadata": {
        "id": "y-YLRFeKK4gv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Apply PCA before logistic regression."
      ]
    },
    {
      "metadata": {
        "id": "ypa-OUNAEYRY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-PBzEkvtEreL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VAka_LAVFV-E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Fitting to raw data takes around 10 minutes and achieves train accuracy of 95.4%\n",
        "## and test accuracy of 77.6%.\n",
        "## - High training time\n",
        "## - Huge overfitting \n",
        "\n",
        "# %%time\n",
        "# lr.fit(fashion_train_X, fashion_train_y)\n",
        "# print(lr.score(fashion_train_X, fashion_train_y),lr.score(fashion_test_X, fashion_test_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iII9PVomIutu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Fitting to PCA'd data is 20X faster and has drastically reduced overfitting.\n",
        "%%time\n",
        "lr.fit(fashion_train_X_pca, fashion_train_y)\n",
        "print(lr.score(fashion_train_X_pca, fashion_train_y),lr.score(fashion_test_X_pca, fashion_test_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z7UYrEoaK8_e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Apply PCA before random forest"
      ]
    },
    {
      "metadata": {
        "id": "6kozDciKLDfF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aufkM7GTLKij",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "41EiG8CDLgtU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "rfc.fit(fashion_train_X, fashion_train_y)\n",
        "print(rfc.score(fashion_train_X, fashion_train_y),rfc.score(fashion_test_X, fashion_test_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7lDaX83kL-Gf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "rfc = RandomForestClassifier()\n",
        "rfc.fit(fashion_train_X_pca, fashion_train_y)\n",
        "print(rfc.score(fashion_train_X_pca, fashion_train_y),rfc.score(fashion_test_X_pca, fashion_test_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aq7bBLewD2Hz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Using PCA based dimensionality reduction for clustering"
      ]
    },
    {
      "metadata": {
        "id": "PRDaXtJqzCzC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hLns1D9jzQOb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "km = KMeans(n_clusters=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D70r2AhMzXWb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cluster_id_fashion_train = km.fit_predict(fashion_train_X)\n",
        "cluster_id_fashion_pca = km.fit_predict(fashion_train_X_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a3r4D4A6z1Jd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_label_count_by_cluster_id(cluster_id,y): \n",
        "  label_count_by_cluster_id = np.zeros([10,10])\n",
        "\n",
        "  for cid in np.unique(cluster_id):\n",
        "    for label in np.unique(y):\n",
        "      label_count_by_cluster_id[cid,label] = np.sum(y[cid == cluster_id] == label)\n",
        "  \n",
        "  return label_count_by_cluster_id\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dx9HvLj-1nog",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.stats import entropy\n",
        "\n",
        "def get_avg_cluster_impurity(counts_by_cluster_id,cluster_ids):\n",
        "  impurity = np.zeros([10])\n",
        "  for cid in np.unique(cluster_ids):\n",
        "    impurity[cid] = entropy(counts_by_cluster_id[cid,:])\n",
        "\n",
        "  return (np.mean(impurity))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RLEQXcyN19bt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "label_count_train = get_label_count_by_cluster_id(cluster_id_fashion_train,fashion_train_y)\n",
        "label_count_pca = get_label_count_by_cluster_id(cluster_id_fashion_pca,fashion_train_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "59mYW7473XQf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Raw')\n",
        "print(label_count_train)\n",
        "print(get_avg_cluster_impurity(label_count_train, cluster_id_fashion_train))\n",
        "\n",
        "print('PCA\\'d')\n",
        "print(label_count_pca)\n",
        "print(get_avg_cluster_impurity(label_count_pca, cluster_id_fashion_pca))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nicQ1wrozCZx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Independent Component Analysis\n",
        "\n",
        "**Motivation:** Use a more powerful measure of redundancy than covariance / correlation.\n",
        "\n",
        "**The Idea of Statistical Independence:** Suppose we are given two vectors (or distributions) resulting from rolling **biased** dice $n$ times, how can we ascertain if the two vectors are produced by two different (differently biased) dice or the same die? The idea is to use probability theory to test if the joint probabilty distribution of the two distributions and the product of their marginal distributions is almost same. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ZvUUICKnOrtL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Testing independence of two distinct dice using joint probability"
      ]
    },
    {
      "metadata": {
        "id": "7FlsQd1EIpt3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "die1_faces = [1,2,3,4,5,6]\n",
        "die1_weights = [0.1, 0.2, 0.2, 0.2, 0.2, 0.1]\n",
        "die2_faces = [1,2,3,4,5,6]\n",
        "die2_weights = [0.09, 0.17, 0.29, 0.19, 0.2, 0.06]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3B8cO3vL58Ri",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Disctinct dice\n",
        "die1_output = np.random.choice(die1_faces,p=die1_weights,size=1000)\n",
        "die2_output = np.random.choice(die2_faces,p=die2_weights,size=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g8eeuvvH895V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "plt.hist(die1_output,bins=die1_faces+[7],color='g',histtype='step')\n",
        "plt.hist(die2_output,bins=die2_faces+[7],color='r',histtype='step')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2PA7NM5L9X6g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "probs1, _ = np.histogram(die1_output,bins=die1_faces+[7], normed=True)\n",
        "probs2, _ = np.histogram(die2_output,bins=die2_faces+[7], normed=True)\n",
        "print(probs1,probs2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1HULULGQB22R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_joint_probability(die1, die2):\n",
        "  joint_probability = np.zeros([6,6])\n",
        "\n",
        "  for a,b in zip(die1, die2):\n",
        "    joint_probability[a-1,b-1] += 1\n",
        "  \n",
        "  joint_probability /= np.sum(joint_probability)\n",
        "  \n",
        "  return joint_probability\n",
        "\n",
        "jp_distinct = get_joint_probability(die1_output, die2_output)\n",
        "print(jp_distinct)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aoNapd9vC1L8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_prod_probability(dist1, dist2):\n",
        "  prod_probability = np.zeros([6,6])\n",
        "  \n",
        "  for i in range(len(dist1)):\n",
        "    for j in range(len(dist2)):\n",
        "      prod_probability[i,j] = dist1[i]*dist2[j]\n",
        "  \n",
        "  return prod_probability\n",
        "\n",
        "pp_distinct = get_prod_probability(probs1,probs2)\n",
        "print(pp_distinct)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZCtLogjbEqWj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.mean(np.abs(jp_distinct - pp_distinct))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-iJ05w9-OJfP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Re-roll first die\n",
        "die1_output_ = np.random.choice(die1_faces,p=die1_weights,size=1000)\n",
        "probs1_ , _ = np.histogram(die1_output_,bins=die1_faces+[7], normed=True)\n",
        "\n",
        "jp_non_distinct = get_joint_probability(die1_output, die1_output_)\n",
        "pp_non_distinct = get_prod_probability(probs1,probs1_)\n",
        "print(jp_non_distinct)\n",
        "print(pp_non_distinct)\n",
        "print(np.mean(np.abs(jp_non_distinct - pp_non_distinct)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DuNf8TsqZEfk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Testing independence of two distinct dice using relative entropy aka  [Kullbackâ€“Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) "
      ]
    },
    {
      "metadata": {
        "id": "Gb2nBcsPOU6s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(entropy(probs1, probs1_))\n",
        "print(entropy(probs1, probs2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b2qsb_3jfoTc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ICA finds new dimensions that maximize the statistical independence between the  new dimensions. \n",
        "\n",
        "Applications of ICA and its variants\n",
        "\n",
        "- Image Reflection Removal [1](https://pdfs.semanticscholar.org/c106/d34429cddab7291e02664025fe59cdae846a.pdf) [2](http://webee.technion.ac.il/people/zeevi/papers/38.pdf.pdf)\n",
        "\n",
        "- Face recognition [1](http://cs229.stanford.edu/proj2007/Rajgarhia-FaceDetectionUsingICA.pdf)\n",
        "\n",
        "- Medical signal analysis (fMRI, ECG, EEG)\n",
        "\n",
        "- Blind source audio separation. [1](http://www.kecl.ntt.co.jp/icl/signal/sawada/mypaper/icassp2007Tutorial11.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "45hP4Lt9i3cf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Blind Source Audio Separation with ICA\n",
        "Let us say we are given three recordings of three persons speaking simultaneously. Each recording is a dimension or feature in the original dataset.\n",
        "ICA can be used to derive new features (or dimensions) that are more statistically independent with each other than the original dimensions.\n",
        "The new dimensions will be mostly pure audio from one person with audio from other persons magically removed."
      ]
    },
    {
      "metadata": {
        "id": "70mrlgPj2XOM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Unmix synthetic audio.\n",
        "We will first mix 3 pure audio recordings in different ratios and generate 3 mixtures. We will then unmix this synthetic mixture using ICA decomposition."
      ]
    },
    {
      "metadata": {
        "id": "R7Q9zgq03uei",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Download pure audio from 3 different speakers.\n",
        "The files are taken from [here](http://www.kecl.ntt.co.jp/icl/signal/sawada/demo/bss2to4/index.html)."
      ]
    },
    {
      "metadata": {
        "id": "DhFGcRjPvnLM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "request.urlretrieve('https://github.com/farhanhubble/sapient-webinars/raw/master/s1.wav', './s1.wav')\n",
        "request.urlretrieve('https://github.com/farhanhubble/sapient-webinars/raw/master/s2.wav', './s2.wav')\n",
        "request.urlretrieve('https://github.com/farhanhubble/sapient-webinars/raw/master/s3.wav', './s3.wav')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sti13PdN39fK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Create synthetic mixtures.\n",
        "For synthetic mixing we just add the audios. To create 3 mixtures we try three different (arbitrary) mixing ratios."
      ]
    },
    {
      "metadata": {
        "id": "8tnIJ1ir94aF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.io import wavfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hKjPtGkzsde-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Create 3 different mixtures by adding the pure audio recordings in different ratios.\n",
        "rate1, s1 = wavfile.read('./s1.wav')\n",
        "rate2, s2 = wavfile.read('./s2.wav')\n",
        "rate3, s3 = wavfile.read('./s3.wav')\n",
        "\n",
        "syn_mix1 = 0.3*s1+0.2*s2+0.5*s3\n",
        "syn_mix2 = 0.2*s1+0.4*s2+0.4*s3\n",
        "syn_mix3 = 0.4*s1+0.3*s2+0.3*s3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B0lvPLlY5ZDO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Listen to the pure recordings"
      ]
    },
    {
      "metadata": {
        "id": "t8dvZRAN98Ao",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jOBcAbbU5ec6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Audio(s1,rate=rate1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aFAMfgA85kZ1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Audio(s2,rate=rate2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Dgnhb6l5noA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Audio(s3,rate=rate3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e-ocF2Ut62nY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Plot the pure audio recordings and their mixtures."
      ]
    },
    {
      "metadata": {
        "id": "80RBTLOh6AB6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.subplot(1,3,1)\n",
        "plt.plot(s1,'r')\n",
        "plt.subplot(1,3,2)\n",
        "plt.plot(s2,'g')\n",
        "plt.subplot(1,3,3)\n",
        "plt.plot(s3,'b')\n",
        "plt.show()\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.plot(syn_mix1,'c')\n",
        "plt.subplot(1,3,2)\n",
        "plt.plot(syn_mix2,'y')\n",
        "plt.subplot(1,3,3)\n",
        "plt.plot(syn_mix3,'m')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f7HNw7i5sx_k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Create a single matrix containing the three mixtures in its columns.\n",
        "syn_audio_mix = np.hstack((syn_mix1.reshape([-1,1]), syn_mix2.reshape([-1,1]), syn_mix3.reshape([-1,1])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kXUbFvZD4gma",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Listen to the mixtures."
      ]
    },
    {
      "metadata": {
        "id": "QL19r_AmtxNa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Audio(syn_mix1,rate=rate1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PKah8IZb1dek",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Audio(syn_mix2,rate=rate2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MUNugHFM1hIG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Audio(syn_mix3,rate=rate3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUIZDYb0-HP9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import FastICA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lfmAgg0a-JWj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ica = FastICA(whiten=True, max_iter=10000, tol=1e-6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pk6TVgRN7Nvl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Unmix the three mixtures to 3 statistically independent components.\n",
        "syn_audio_unmixed = ica.fit_transform(syn_audio_mix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-IJ9DOBQ7nW1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Listen to unmixed components.\n",
        "Note that there is no particular order in which the components are generated by ICA. "
      ]
    },
    {
      "metadata": {
        "id": "6Pec63fJt31_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Audio(syn_audio_unmixed[:,0], rate=rate3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4E8StjdzuGG_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Audio(syn_audio_unmixed[:,1], rate=rate3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8XTO9QgBuc5K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Audio(syn_audio_unmixed[:,2], rate=rate3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NFkSjlQB8W8N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Unmix real-world audio.\n",
        "Download 3 audio files, each of which is a mixture of audio (speech) of 3 speakers. The files are avialable [here]( http://www.kecl.ntt.co.jp/icl/signal/sawada/demo/bss2to4/index.html). Unlike the synthetic mixtures, these mixtures were produced by playing three audios in a room and then recording the audio in the room with three different microphones. This mixture is not a simple element-wise sum, rather a convolutive sum as direct audio is mixed with multiple reverberated and delayed audio signals. \n",
        "\n",
        "See [this](http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/4924/pdf/imm4924.pdf) paper for different types of audio mixing. "
      ]
    },
    {
      "metadata": {
        "id": "msRz0Ua4OiC5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "audio_mix1_url = 'https://github.com/farhanhubble/sapient-webinars/raw/master/x3-1.wav'\n",
        "audio_mix2_url = 'https://github.com/farhanhubble/sapient-webinars/raw/master/x3-2.wav'\n",
        "audio_mix3_url = 'https://github.com/farhanhubble/sapient-webinars/raw/master/x3-3.wav'\n",
        "\n",
        "request.urlretrieve(audio_mix1_url, './mix1.wav')\n",
        "request.urlretrieve(audio_mix2_url, './mix2.wav')\n",
        "request.urlretrieve(audio_mix3_url, './mix3.wav')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kiNuto7Cm8Rj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rate1, mix1 = wavfile.read('./mix1.wav')\n",
        "rate2, mix2 = wavfile.read('./mix2.wav')\n",
        "rate3, mix3 = wavfile.read('./mix3.wav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ew_hRMUQne_C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Audio(mix1, rate=rate1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h-OsfDnOpZHJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Audio(mix2, rate=rate2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lviv-RbgqpGE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Audio(mix3, rate=rate3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zjm6oVr3tb1p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "audio_mixed = np.hstack((mix1.reshape([-1,1]), mix2.reshape([-1,1]), mix3.reshape([-1,1])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RzaTPaOltCwY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "audio_unmixed = ica.fit_transform(audio_mixed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JdFguQAGth18",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Audio(audio_unmixed[:,0], rate=rate3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-7ONbqzntib_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Audio(audio_unmixed[:,1], rate=rate3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EbADWc13vlTH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Audio(audio_unmixed[:,2], rate=rate3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GFTWV3dABC_c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ICA caveats\n",
        "\n",
        "- Unlike PCA, in ICA the number of components has to be guessed.\n",
        "- For audio decomposition using ICA the number of features/mixtures should be equal to the number of pure sources. \n",
        "- For real-world audio decomposition, the mixtures should be first transformed to the frequency domain using short-time frequency transform(STFT)."
      ]
    },
    {
      "metadata": {
        "id": "YqedmdUUKqX_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### How is ICA a decomposition?\n",
        "The mixing process is \n",
        "$X = M \\cdot A$\n",
        "\n",
        "ICA takes a matrix of mixtures $X$ and finds an unmixing matrix $U$ such that $U*X \\simeq A$, essentially decomposing a mixture into a mixing matrix and unmixed signals."
      ]
    },
    {
      "metadata": {
        "id": "UXzBaCDKOGD_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Non-negative Matrix Factorization (NMF)\n",
        "PCA tries to minimize linear dependence or correlation between features/dimensions while\n",
        "ICA tries to maximize the, much more stringent, statistical independence between features. NMA on the other hand uses a very relaxed condition to decompose a matrix $V_{m \\times n}$ into a product of two matrices $W_{m \\times k} \\cdot H_{k \\times n}$, such that every element in both $W$ and $H$ is non-negative and the intermediate dimenskion $k << n$ (sparsity).\n",
        "\n",
        "NMF is used in [clustering](https://pdfs.semanticscholar.org/cbb3/1694c3ca105c618c06374475ceb3aa80f9db.pdf) and [collaborative filtering](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.415&rep=rep1&type=pdf)."
      ]
    },
    {
      "metadata": {
        "id": "Z6IuRa2zQrzL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import NMF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yhJ0ko2sZ-Tz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nmf = NMF()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wgmGy4b1aJev",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "W = nmf.fit_transform(fashion_test_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_NEgaa2Nab6k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}